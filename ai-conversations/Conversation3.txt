AI - conversation
------------------

Tool :- ChatGPT
Date :- 18-10-2025

Me :
----

Modify the code such that, instead of giving a JSON output I need to get the output as a humanise response (like The weather today is....)
and implement the visualisation part ( displaying graphs of temperature, precipitaion )

AI Response :
--------------

import requests
import matplotlib.pyplot as plt
import json
import ollama
import re

# ------------------- WEATHER FETCHING -------------------

def get_weather_data(location):
    url = f"https://wttr.in/{location}?format=j1"
    response = requests.get(url)
    response.raise_for_status()
    return response.json()

# ------------------- LLAMA INTEGRATION -------------------

def llama_parse_question(question):
    prompt = (
        "Extract the location, time period, and weather attribute from the following question. "
        "Return ONLY a JSON object with keys: location, time_period, attribute. "
        "If not clear, use 'today' for time_period and 'temperature' for attribute. "
        "Do not include explanations or markdown.\n"
        f"Question: {question}\n"
        "JSON:"
    )
    try:
        response = ollama.chat(model="llama3", messages=[{"role": "user", "content": prompt}])
        return response["message"]["content"]
    except Exception as e:
        print(f"‚ö†Ô∏è Error calling Ollama: {e}")
        return None

def extract_json_from_llama_response(response_text):
    response_text = response_text.strip()
    if response_text.startswith("```json"):
        response_text = response_text[7:]
    if response_text.startswith("```"):
        response_text = response_text[3:]
    if response_text.endswith("```"):
        response_text = response_text[:-3]
    match = re.search(r"\{.*\}", response_text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group())
        except Exception as e:
            print("DEBUG: JSON decode error:", e)
    return None

def llama_generate_response(weather_info, question):
    """
    Generate a natural response using Ollama.
    If Ollama isn't available, fall back to a formatted human sentence.
    """
    prompt = (
        f"Given this weather data:\n{weather_info}\n"
        f"Answer the user's question naturally and clearly:\n{question}\n"
        "Example: 'The weather today in Sydney is sunny with 28¬∞C and mild winds.'\n"
        "Response:"
    )
    try:
        response = ollama.chat(model="llama3", messages=[{"role": "user", "content": prompt}])
        if isinstance(response, dict):
            return response.get("message", {}).get("content", "")
        elif isinstance(response, list) and len(response) > 0:
            return response[0].get("message", {}).get("content", "")
        else:
            return ""
    except Exception as e:
        print(f"‚ö†Ô∏è Ollama unavailable, using fallback description: {e}")
        return weather_info

# ------------------- LOCAL HUMANIZED OUTPUT -------------------

def humanize_weather(weather_data, location, time_period="today"):
    """
    Generate a human-readable weather summary without using Ollama.
    """
    if time_period.lower() in ["today", "current"]:
        current = weather_data["current_condition"][0]
        desc = current["weatherDesc"][0]["value"]
        temp = current["temp_C"]
        feels_like = current["FeelsLikeC"]
        humidity = current["humidity"]
        precip = current["precipMM"]
        return (
            f"The weather today in {location.title()} is {desc.lower()} "
            f"with a temperature of {temp}¬∞C (feels like {feels_like}¬∞C), "
            f"humidity around {humidity}%, and precipitation of {precip} mm."
        )
    else:
        forecasts = []
        for day in weather_data["weather"][:3]:
            date = day["date"]
            avgtemp = day["avgtempC"]
            desc = day["hourly"][0]["weatherDesc"][0]["value"]
            rain_chance = day["hourly"][0]["chanceofrain"]
            forecasts.append(
                f"On {date}, expect {desc.lower()} with an average temperature of {avgtemp}¬∞C "
                f"and a {rain_chance}% chance of rain."
            )
        return f"Here‚Äôs the {time_period} forecast for {location.title()}:\n" + "\n".join(forecasts)

# ------------------- VISUALIZATION -------------------

def plot_temperature_trend(weather_data, location):
    """Plot the average temperature for the next 3 days."""
    dates = [day["date"] for day in weather_data["weather"][:3]]
    temps = [int(day["avgtempC"]) for day in weather_data["weather"][:3]]

    plt.figure(figsize=(6, 4))
    plt.plot(dates, temps, marker='o', linestyle='-', label="Avg Temperature (¬∞C)")
    plt.title(f"Temperature Trend - {location.title()}")
    plt.xlabel("Date")
    plt.ylabel("Temperature (¬∞C)")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

def plot_precipitation_trend(weather_data, location):
    """Plot the average precipitation for the next 3 days."""
    dates = [day["date"] for day in weather_data["weather"][:3]]
    precip_values = []
    for day in weather_data["weather"][:3]:
        hourly = day["hourly"]
        avg_precip = sum(float(h["precipMM"]) for h in hourly) / len(hourly)
        precip_values.append(avg_precip)

    plt.figure(figsize=(6, 4))
    plt.bar(dates, precip_values, color="skyblue", edgecolor="black")
    plt.title(f"Precipitation Trend - {location.title()}")
    plt.xlabel("Date")
    plt.ylabel("Avg Precipitation (mm)")
    plt.tight_layout()
    plt.show()

# ------------------- MAIN EXECUTION -------------------

def main():
    print("Ask your weather question (e.g., 'How is today's weather in Perth'):")
    question = input("Question: ")

    parsed_text = llama_parse_question(question)
    print("\nDEBUG: Raw Llama response:\n", parsed_text)

    if not parsed_text or parsed_text.strip() == "":
        print("‚ùå Error: Llama did not return any response.")
        return

    info = extract_json_from_llama_response(parsed_text)
    if not info:
        print("‚ùå Error: Invalid JSON returned from Llama.")
        print(parsed_text)
        return

    location = info.get("location", "").strip()
    time_period = info.get("time_period", "today").strip()
    if not location:
        print("‚ùå Could not determine location from question.")
        return

    # Fetch weather data
    weather_data = get_weather_data(location)

    # Humanized weather description
    local_summary = humanize_weather(weather_data, location, time_period)

    # Optional AI-enhanced response
    response = llama_generate_response(local_summary, question)
    if not response.strip():
        response = local_summary

    print("\n‚úÖ Final Humanized Weather Response:\n")
    print(response)

    # ---- Visualization Section ----
    print("\nüìä Generating Weather Visualizations...")
    plot_temperature_trend(weather_data, location)
    plot_precipitation_trend(weather_data, location)

if __name__ == "__main__":
    main()


------------------------------------------------------------------------------------------------------------------------------------

Me : 
-----

Modify the  code such that, after user gives an input then give the humanise response and after that user should have an 
option to select if he needs visualisation or not ( if he needs visualisation then he should select "y" and of 
he don't need it then he should select "n" and after selecting n the programe should exit


AI Response :
-------------

import requests
import matplotlib.pyplot as plt
import json
import ollama
import re

# ------------------- WEATHER FETCHING -------------------

def get_weather_data(location):
    url = f"https://wttr.in/{location}?format=j1"
    response = requests.get(url)
    response.raise_for_status()
    return response.json()

# ------------------- LLAMA INTEGRATION -------------------

def llama_parse_question(question):
    prompt = (
        "Extract the location, time period, and weather attribute from the following question. "
        "Return ONLY a JSON object with keys: location, time_period, attribute. "
        "If not clear, use 'today' for time_period and 'temperature' for attribute. "
        "Do not include explanations or markdown.\n"
        f"Question: {question}\n"
        "JSON:"
    )
    try:
        response = ollama.chat(model="llama3", messages=[{"role": "user", "content": prompt}])
        return response["message"]["content"]
    except Exception as e:
        print(f"‚ö†Ô∏è Error calling Ollama: {e}")
        return None

def extract_json_from_llama_response(response_text):
    response_text = response_text.strip()
    if response_text.startswith("```json"):
        response_text = response_text[7:]
    if response_text.startswith("```"):
        response_text = response_text[3:]
    if response_text.endswith("```"):
        response_text = response_text[:-3]
    match = re.search(r"\{.*\}", response_text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group())
        except Exception as e:
            print("DEBUG: JSON decode error:", e)
    return None

def llama_generate_response(weather_info, question):
    """
    Generate a natural response using Ollama.
    If Ollama isn't available, fall back to a formatted human sentence.
    """
    prompt = (
        f"Given this weather data:\n{weather_info}\n"
        f"Answer the user's question naturally and clearly:\n{question}\n"
        "Example: 'The weather today in Sydney is sunny with 28¬∞C and mild winds.'\n"
        "Response:"
    )
    try:
        response = ollama.chat(model="llama3", messages=[{"role": "user", "content": prompt}])
        if isinstance(response, dict):
            return response.get("message", {}).get("content", "")
        elif isinstance(response, list) and len(response) > 0:
            return response[0].get("message", {}).get("content", "")
        else:
            return ""
    except Exception as e:
        print(f"‚ö†Ô∏è Ollama unavailable, using fallback description: {e}")
        return weather_info

# ------------------- LOCAL HUMANIZED OUTPUT -------------------

def humanize_weather(weather_data, location, time_period="today"):
    """
    Generate a human-readable weather summary without using Ollama.
    """
    if time_period.lower() in ["today", "current"]:
        current = weather_data["current_condition"][0]
        desc = current["weatherDesc"][0]["value"]
        temp = current["temp_C"]
        feels_like = current["FeelsLikeC"]
        humidity = current["humidity"]
        precip = current["precipMM"]
        return (
            f"The weather today in {location.title()} is {desc.lower()} "
            f"with a temperature of {temp}¬∞C (feels like {feels_like}¬∞C), "
            f"humidity around {humidity}%, and precipitation of {precip} mm."
        )
    else:
        forecasts = []
        for day in weather_data["weather"][:3]:
            date = day["date"]
            avgtemp = day["avgtempC"]
            desc = day["hourly"][0]["weatherDesc"][0]["value"]
            rain_chance = day["hourly"][0]["chanceofrain"]
            forecasts.append(
                f"On {date}, expect {desc.lower()} with an average temperature of {avgtemp}¬∞C "
                f"and a {rain_chance}% chance of rain."
            )
        return f"Here‚Äôs the {time_period} forecast for {location.title()}:\n" + "\n".join(forecasts)

# ------------------- VISUALIZATION -------------------

def plot_temperature_trend(weather_data, location):
    """Plot the average temperature for the next 3 days."""
    dates = [day["date"] for day in weather_data["weather"][:3]]
    temps = [int(day["avgtempC"]) for day in weather_data["weather"][:3]]

    plt.figure(figsize=(6, 4))
    plt.plot(dates, temps, marker='o', linestyle='-', label="Avg Temperature (¬∞C)")
    plt.title(f"Temperature Trend - {location.title()}")
    plt.xlabel("Date")
    plt.ylabel("Temperature (¬∞C)")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

def plot_precipitation_trend(weather_data, location):
    """Plot the average precipitation for the next 3 days."""
    dates = [day["date"] for day in weather_data["weather"][:3]]
    precip_values = []
    for day in weather_data["weather"][:3]:
        hourly = day["hourly"]
        avg_precip = sum(float(h["precipMM"]) for h in hourly) / len(hourly)
        precip_values.append(avg_precip)

    plt.figure(figsize=(6, 4))
    plt.bar(dates, precip_values, color="skyblue", edgecolor="black")
    plt.title(f"Precipitation Trend - {location.title()}")
    plt.xlabel("Date")
    plt.ylabel("Avg Precipitation (mm)")
    plt.tight_layout()
    plt.show()

# ------------------- MAIN EXECUTION -------------------

def main():
    print("Ask your weather question (e.g., 'How is today's weather in Perth'):")
    question = input("Question: ")

    parsed_text = llama_parse_question(question)
    print("\nDEBUG: Raw Llama response:\n", parsed_text)

    if not parsed_text or parsed_text.strip() == "":
        print("‚ùå Error: Llama did not return any response.")
        return

    info = extract_json_from_llama_response(parsed_text)
    if not info:
        print("‚ùå Error: Invalid JSON returned from Llama.")
        print(parsed_text)
        return

    location = info.get("location", "").strip()
    time_period = info.get("time_period", "today").strip()
    if not location:
        print("‚ùå Could not determine location from question.")
        return

    # Fetch weather data
    weather_data = get_weather_data(location)

    # Humanized weather description
    local_summary = humanize_weather(weather_data, location, time_period)

    # Optional AI-enhanced response
    response = llama_generate_response(local_summary, question)
    if not response.strip():
        response = local_summary

    print("\n‚úÖ Final Humanized Weather Response:\n")
    print(response)

    # ---- Visualization Section ----
    print("\nüìä Generating Weather Visualizations...")
    plot_temperature_trend(weather_data, location)
    plot_precipitation_trend(weather_data, location)

if __name__ == "__main__":
    main()


AI RESPONSE :

import requests
import matplotlib.pyplot as plt
import json
import ollama
import re
import sys

# ------------------- WEATHER FETCHING -------------------

def get_weather_data(location):
    url = f"https://wttr.in/{location}?format=j1"
    response = requests.get(url)
    response.raise_for_status()
    return response.json()

# ------------------- LLAMA INTEGRATION -------------------

def llama_parse_question(question):
    prompt = (
        "Extract the location, time period, and weather attribute from the following question. "
        "Return ONLY a JSON object with keys: location, time_period, attribute. "
        "If not clear, use 'today' for time_period and 'temperature' for attribute. "
        "Do not include explanations or markdown.\n"
        f"Question: {question}\n"
        "JSON:"
    )
    try:
        response = ollama.chat(model="llama3", messages=[{"role": "user", "content": prompt}])
        return response["message"]["content"]
    except Exception as e:
        print(f"‚ö†Ô∏è Error calling Ollama: {e}")
        return None

def extract_json_from_llama_response(response_text):
    response_text = response_text.strip()
    if response_text.startswith("```json"):
        response_text = response_text[7:]
    if response_text.startswith("```"):
        response_text = response_text[3:]
    if response_text.endswith("```"):
        response_text = response_text[:-3]
    match = re.search(r"\{.*\}", response_text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group())
        except Exception as e:
            print("DEBUG: JSON decode error:", e)
    return None

def llama_generate_response(weather_info, question):
    """
    Generate a natural response using Ollama.
    If Ollama isn't available, fall back to a formatted human sentence.
    """
    prompt = (
        f"Given this weather data:\n{weather_info}\n"
        f"Answer the user's question naturally and clearly:\n{question}\n"
        "Example: 'The weather today in Sydney is sunny with 28¬∞C and mild winds.'\n"
        "Response:"
    )
    try:
        response = ollama.chat(model="llama3", messages=[{"role": "user", "content": prompt}])
        if isinstance(response, dict):
            return response.get("message", {}).get("content", "")
        elif isinstance(response, list) and len(response) > 0:
            return response[0].get("message", {}).get("content", "")
        else:
            return ""
    except Exception as e:
        print(f"‚ö†Ô∏è Ollama unavailable, using fallback description: {e}")
        return weather_info

# ------------------- LOCAL HUMANIZED OUTPUT -------------------

def humanize_weather(weather_data, location, time_period="today"):
    """Generate a human-readable weather summary without using Ollama."""
    if time_period.lower() in ["today", "current"]:
        current = weather_data["current_condition"][0]
        desc = current["weatherDesc"][0]["value"]
        temp = current["temp_C"]
        feels_like = current["FeelsLikeC"]
        humidity = current["humidity"]
        precip = current["precipMM"]
        return (
            f"The weather today in {location.title()} is {desc.lower()} "
            f"with a temperature of {temp}¬∞C (feels like {feels_like}¬∞C), "
            f"humidity around {humidity}%, and precipitation of {precip} mm."
        )
    else:
        forecasts = []
        for day in weather_data["weather"][:3]:
            date = day["date"]
            avgtemp = day["avgtempC"]
            desc = day["hourly"][0]["weatherDesc"][0]["value"]
            rain_chance = day["hourly"][0]["chanceofrain"]
            forecasts.append(
                f"On {date}, expect {desc.lower()} with an average temperature of {avgtemp}¬∞C "
                f"and a {rain_chance}% chance of rain."
            )
        return f"Here‚Äôs the {time_period} forecast for {location.title()}:\n" + "\n".join(forecasts)

# ------------------- VISUALIZATION -------------------

def plot_temperature_trend(weather_data, location):
    """Plot the average temperature for the next 3 days."""
    dates = [day["date"] for day in weather_data["weather"][:3]]
    temps = [int(day["avgtempC"]) for day in weather_data["weather"][:3]]

    plt.figure(figsize=(6, 4))
    plt.plot(dates, temps, marker='o', linestyle='-', label="Avg Temperature (¬∞C)")
    plt.title(f"Temperature Trend - {location.title()}")
    plt.xlabel("Date")
    plt.ylabel("Temperature (¬∞C)")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

def plot_precipitation_trend(weather_data, location):
    """Plot the average precipitation for the next 3 days."""
    dates = [day["date"] for day in weather_data["weather"][:3]]
    precip_values = []
    for day in weather_data["weather"][:3]:
        hourly = day["hourly"]
        avg_precip = sum(float(h["precipMM"]) for h in hourly) / len(hourly)
        precip_values.append(avg_precip)

    plt.figure(figsize=(6, 4))
    plt.bar(dates, precip_values, color="skyblue", edgecolor="black")
    plt.title(f"Precipitation Trend - {location.title()}")
    plt.xlabel("Date")
    plt.ylabel("Avg Precipitation (mm)")
    plt.tight_layout()
    plt.show()

# ------------------- MAIN EXECUTION -------------------

def main():
    print("Ask your weather question (e.g., 'How is today's weather in Perth'):")
    question = input("Question: ")

    parsed_text = llama_parse_question(question)
    if not parsed_text or parsed_text.strip() == "":
        print("‚ùå Error: Llama did not return any response.")
        sys.exit()

    info = extract_json_from_llama_response(parsed_text)
    if not info:
        print("‚ùå Error: Invalid JSON returned from Llama.")
        print(parsed_text)
        sys.exit()

    location = info.get("location", "").strip()
    time_period = info.get("time_period", "today").strip()
    if not location:
        print("‚ùå Could not determine location from question.")
        sys.exit()

    # Fetch weather data
    weather_data = get_weather_data(location)

    # Humanized weather description
    local_summary = humanize_weather(weather_data, location, time_period)

    # Optional AI-enhanced response
    response = llama_generate_response(local_summary, question)
    if not response.strip():
        response = local_summary

    print("\n‚úÖ Final Humanized Weather Response:\n")
    print(response)

    # ---- Visualization Option ----
    choice = input("\nWould you like to see the weather visualizations? (y/n): ").strip().lower()

    if choice == 'y':
        print("\nüìä Generating Weather Visualizations...")
        plot_temperature_trend(weather_data, location)
        plot_precipitation_trend(weather_data, location)
    else:
        print("\nüëã Exiting program. Have a nice day!")
        sys.exit()

if __name__ == "__main__":
    main()


